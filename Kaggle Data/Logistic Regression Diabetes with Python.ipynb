{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Contents\n\n> ### [Logistic Regression Intro ](#s-1)\n> ### 1. [Load and Read Data](#s-2)\n> ### 2. [Normalization](#s-3)\n> ### 3. [Traine and Test Split](#s-4)\n> ### 4. [Functions](#s-5)\n   >> ### 4.1. [Initializing Parameters](#s-6)\n   >> ### 4.2. [Forward Propagation](#s-7)\n   >> ### 4.3. [Optimization Algorithm with Gradient Descent](#s-8)\n   >> ### 4.4. [Backward Propagation](#s-9)\n   >> ### 4.5. [Updating (Learning) Parameters](#s-10)\n   >> ### 4.6. [Prediction](#s-11)\n   >> ### 4.6. [Defining Logistic Regression Function ](#s-12)\n> ### 5. [With Sklearn](#s-13)\n"},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Intro\n\n* İstatistiklerde, lojistik model, başarılı / başarısız, kazan / kaybet, canlı / ölü veya sağlıklı / hasta gibi belirli bir sınıf veya olayın olasılığını modellemek için kullanılır. Bu, bir görüntünün kedi, köpek, aslan vb. İçerip içermediğini belirleme gibi çeşitli olay sınıflarını modellemek için genişletilebilir. Görüntüde algılanan her nesneye 0 ile 1 arasında bir olasılık ve bir toplamı ekleme olasılığı atanır.\n* Binary sınıflandırma için en iyi modeldir.\n* Ayrıca Lojistik Regresyon derin öğrenmenin çok temel bir şeklidir."},{"metadata":{},"cell_type":"markdown","source":"![LogReg.png](https://i.ibb.co/9Hvb2Yf/5.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Yukarıdaki grafik Logistic Regression'un Computational Graps'ıdır.\nBu grafikte;\n\n* x_train'de 4096 tane piksel olduğunu görüyoruz. Her pikselin kendine ait weightleri var. Pikselleri kendi weightleri ile çarpıp, tüm çarpımları toplayalım ve biası ekleyelim. Bu bize z değerini verecektir.\n\n### z = b + px1w1 + px2w2 + ... + px4096*w4096\n\n* z değerini Sigmoid fonksiyona verip hesaplama yapıyoruz. \n> *Peki Sigmoid fonksiyon nedir?*\n> Sigmoid Function : Verdiğimiz z değerini 0 ile 1 arasında bir değere eşitler. Bu değer probabilistik bir değerdir. Sigmoid func. türevi alınabilen bir fonksiyondur. Türevi alınabilir olması sayesinde weight ile bias 'ı güncelleyebiliyoruz. \n\n> y_head = Sigmoid(z)\n    \n* Sigmoid fonksiyonunun çıktısı olan y_head değeri ile modelimizin loss değerini hesaplarız.\n\n* Tüm loss değerlerinin toplamı Cost değerini verir. Cost değeri yüksek olması modelin kötü olduğunu gösterir\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# ** Machine Learning for Diabetes with Python **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebraa\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-2\"></a>\n## 1. Load and Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/machine-learning-for-diabetes-with-python/diabetes_data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.info()\n#data seti 768 satır veriden  oluşmaktadır","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y ve x_data'yı hazırlama\ny = df.Outcome.values\nx_data = df.drop([\"Outcome\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-3\"></a>\n## 2. Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization; değerleri 0-1 arasında yapmayı sağlar\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Datamızda Logistic Regression modelini kullanarak matematik bir denklem olan modelimizi elde ettik. \nModelimizi hem train hem de test edecek datamız olmalı.\n\n> Traine test split"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-4\"></a>\n## 3. Train Test Split"},{"metadata":{},"cell_type":"markdown","source":"Traine test split ile datamızı belirlediğimiz oranda train data ve test data olarak ayıracağız."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datamız trainde 614 ,testte 154 olmak üzere ayrılmıştır."},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = datasetimizde outcome dışındaki verilerimiz \nx_train.shape\n\n# 614 tane sample\n# 8 tane feature var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transposeunu alma\nx_train = x_train.T\nx_test = x_test.T\ny_train= y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-5\"></a>\n## 4. Functions"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-6\"></a>\n> ## 4.1. Initializing Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize weights and bias\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n#w,b = initialize_weights_and_bias(4096)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-7\"></a>\n> ## 4.2. Forward Propagation\n\n### Forward Propagation;\n* Z değerinin bulunmasını,\n    > z = (p1.w1)+(p2.w2)+(p3.w3)+..+(p4096.w4096)+ bias\n* Z değerini sigmoid funtiona sokarak değer bulunmasını,\n* Loss func hesaplamasını,\n* Cost func hesaplamasını (sum(all loss)) içerir."},{"metadata":{"trusted":true},"cell_type":"code","source":"#z değerinin bulunması\n#z = np.dot(w.T,x_train)+b\n\n# z'nin Sigmoid funksiyona sokulması\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loss Function: \n![loss.func](https://image.ibb.co/eC0JCK/duzeltme.jpg)"},{"metadata":{},"cell_type":"markdown","source":" ### Forward propagation steps:\n * find z = w.T*x+b\n * y_head = sigmoid(z)\n * loss(error) = loss(y,y_head)\n * cost = sum(loss)\n \n#### 4.4. bölümünde fonksiyon güncellencektir."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-8\"></a>\n> ## 4.3. Optimization Algorithm with Gradient Descent\n\nŞuan cost değerini biliyoruz.Cost değerimiz yüksekse bunu düşürmemiz lazım. (Yüksek Cost değeri modelin kötü olduğunu gösterir.)\nCostu azaltmak için weights ve bias'ı güncellemek gerekiyor.\n> Modelimiz cost func en aza indiren weight ve bias parametlerini öğrenmesi gerekir.\n\n> Bu tekniğin adı Gradient Descent Algoritmasıdır.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-9\"></a>\n > ## 4.4. Backward Propagation "},{"metadata":{},"cell_type":"markdown","source":"Gradient descent algortiması ile geriye dönerek weight ve biası güncellemeye Backward Propagation denir.\nBackward Propgtion metodunuda ekleyerek Forward Propagation metodunu güncelleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward Propagation & Backward Propagation Methods\n\ndef forward_backward_propagation(w,b,x_train,y_head):\n    \n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss)) / x_train.shape[1]\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bu noktaya kadar şunları öğrendik;\n* Initializing parameters\n* Finding Cost with Forward Propagation\n* Updating (learning) parameters (weights - bias)\n\nŞimdi güncellenen weight ve biası implement edelim."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-10\"></a>\n > ## 4.5. Updating (Learning) Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating(learning) parameters\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost)) #if section defined to print our cost values in every 10 iteration. We do not need to do that. It's optional.\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-11\"></a>\n> ## 4.6. Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is one means has diabete (y_head=1),\n    # if z is smaller than 0.5, our prediction is zero means does not have diabete (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n#predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-11\"></a>\n> ## 4.7. Defining Logistic Regression Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    # Print train/test Errors\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 4, num_iterations = 120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Learning rate ve iterations sayılarını ayarlayarak doğruluk oranı arttırılabilinir. (accurarcy) \n* Ancak belirli bir noktadan sonra doğruluğunuz değişmez. Cost graphda bunu gözlemleyebilirsiniz.\n* Grafiğin türevi (eğim), artan iteration sayısı ile azalmaktadır (eğimin pozitif olduğunu varsayalım). Bu nedenle, belirli miktarda yinelemeden sonra maliyet fonksiyonu azalmayacaktır.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s-13\"></a>\n## 5. With Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}